# Awasome-Data-Free-Transformer-Pruning [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

## Conference Publications

| Year | Venue | Title | Code |
|:-----|:------:|:----| :----|
| 2024 | `CVPR` | [Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers](https://arxiv.org/abs/2305.17328) | [PyTorch](https://jha-lab.github.io/zerotprune/) (soon ...) |
| 2024 | `ICLR` | [The Need for Speed: Pruning Transformers with One Recipe](https://openreview.net/forum?id=MVmT6uQ3cQ&noteId=yo4VpncFje) | [PyTorch](https://github.com/Skhaki18/optin-transformer-pruning) |
| 2023 | `ICLR` | [Token Merging: Your ViT But Faster](https://openreview.net/forum?id=JroZRaRw7Eu) | [PyTorch](https://github.com/facebookresearch/ToMe) |
| 2022 | `ECCV` | [Adaptive Token Sampling for Efficient Vision Transformers](https://link.springer.com/chapter/10.1007/978-3-031-20083-0_24) | [PyTorch](https://adaptivetokensampling.github.io/) |

## Journal Publications

| Year | Venue | Title | Code |
|:-----|:------:|:----| :----|
